{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d8wsZNA-pufR"
   },
   "source": [
    "# Modelo\n",
    "\n",
    "Dado un vector de características de una imagen de una lesión cutánea, se quiere clasificarlo en benigno o maligno. Se tiene en principio un conjunto de vectores ya caracterizados  $\\{x_1,x_2,\\dots x_m,x_{m+1},\\dots x_n\\}$ , donde cada vector representa las características de la imagen, y cada vector tiene asociado un $y_i \\in \\{-1,1\\}$ donde $-1$ representa que la imagen es benigna, y $1$ que la imagen es maligna. Supongamos que $x_1,…x_m$ corresponde a los vectores con $y_i=-1$, y los vectores $x_{m+1},\\dots x_n$ corresponde a los vectores con $y_i=1$. \n",
    "\n",
    "Para clasificar la imagen en benigna o maligna, primero suponemos que el grupo de vectores benignos (etiquetados con $-1$) es linealmente separable del grupo de vectores malignos (etiquetados con $1$), es decir, es posible encontrar un hiperplano que los separa. Este hiperplano que queremos hallar es el hiperplano que mejor separe estos dos grupos en terminos de maximizar un margen, y, además, que para nuevos vectores logremos clasificarlos de acuerdo a si se encuentran a un lado o al otro del hiperplano.\n",
    "\n",
    "<img src=\"margin.jpg\"  height=\"500\" width=\"500\"/>\n",
    "\n",
    "Tomemos este margen, como el margen funcional definido por $\\zeta_i=y_i(w^Tx_i+b)$, sin perdida de generalidad supongamos que el hiperplano separa a los 2 grupos de tal manera que $$w^Tx_i+b < 0 \\ \\ para \\ i=-1$$\n",
    "$$w^Tx_i+b > 0 \\ \\ para \\ i=1$$\n",
    "\n",
    "Lo que se quiere maximizar es el mínimo de los margenes para cada $x_i$, es decir: \n",
    "\n",
    "\n",
    "$$\\begin{align}\n",
    "& maximizar \\ \\zeta \\\\\n",
    "&sujeto \\ a \\\\\n",
    "&y_i(w^Tx_i+b) \\geq \\zeta \\ \\ para \\ i=1,\\dots n \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Sin embargo, esta aproximación no es muy útil, pues al escalar es posible incrementar $\\zeta$ tanto como se desee, por esta razón usamos el margen geométrico, definido como $\\Gamma_i=\\frac{y_i(w^Tx_i+b)}{\\| w\\|}$, el cual ya es invariante respecto a escalamientos. Tenemos ahora el siguiente planteamiento:\n",
    "\n",
    "$$\\begin{align}\n",
    "&maximizar \\  \\Gamma \\\\\n",
    "&sujeto \\ a \\\\\n",
    "&\\Gamma_i \\geq \\Gamma \\ \\ para \\ i=1,\\dots n \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Lo que es equivalente a\n",
    "\n",
    "$$\\begin{align}\n",
    "&maximizar \\  \\frac{\\zeta}{\\|w\\|} \\\\\n",
    "&sujeto \\ a \\\\\n",
    "&y_i(w^Tx_i+b) \\geq \\zeta \\ \\ para \\ i=1,\\dots n \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "Como vimos, el margen funcional se puede escalar, por lo tanto es posible normalizar nuestro problema\n",
    "\n",
    "$$\\begin{align}\n",
    "&maximizar \\  \\frac{1}{\\|w\\|} \\\\\n",
    "&sujeto \\ a \\\\\n",
    "&y_i(w^Tx_i+b) \\geq 1 \\ \\ para \\ i=1,\\dots n \\\\\n",
    "\\end{align}$$\n",
    "Una interpretación gráfica sería la siguiente\n",
    "\n",
    "<img src=\"m2.png\"  height=\"400\" width=\"400\" margin=\"20px\">\n",
    "\n",
    "Ahora, formalizando nuestro problema para expresarlo como un problema de optimización y llevandolo a la forma estandar, obtenemos:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\min_{w,b} f(w)= \\|w\\| \\\\\n",
    "&sujeto \\ a \\\\\n",
    "&f_i(w)=1-y_i(w^Tx_i+b)\\leq 0 \\ \\ para \\ i=1,\\dots n \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "El siguiente es un planteamiento equivalente:\n",
    "\n",
    "$$\\begin{align}\n",
    "&\\min_{w,b} f(w)= \\frac{\\|w\\|^2}{2} \\\\\n",
    "&sujeto \\ a \\\\\n",
    "&f_i(w)=1-y_i(w^Tx_i+b)\\leq 0 \\ \\ para \\ i=1,\\dots n \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "De esta manera tendríamos un problema de optimización convexa $\\textbf{QP}$ donde la función de costo es $f(w)$, la cual es convexa y cuadrática, con $n$ condiciones de desigualdad $f_i$ las cuales son funciones afines, como estas condiciones de desigualdad son afines, entonces se satisface la condición de Slater refinada, con lo cual se cumple dualidad fuerte. Ahora veamos el problema de dualidad.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECP03iJb8WOA"
   },
   "source": [
    "### Problema dual\n",
    "#### Lagrangiano $\\mathcal{L}(w,b,\\lambda)$\n",
    "\n",
    "Como únicamente tenemos condiciones de desigualdad, entonces el lagrangiano está dado por\n",
    "$$\\mathcal{L}(w,b,\\lambda)=\\frac{1}{2}\\|w\\|^2- \\sum_{i=1}^n \\lambda_i(y_i(w^Tx_i+b)-1) \\ \\ \\, \\ \\lambda_i\\geq 0$$\n",
    "\n",
    "\n",
    "#### Función dual $g(\\lambda)$\n",
    "$$g(\\lambda)=\\inf_{w,b} \\mathcal{L}(w,b,\\lambda)$$\n",
    "\n",
    "Denotando $w^*,b^*,\\lambda^* $ como las soluciones al problema de optimización (recordemos que el problema dual se presenta en terminos de $\\lambda$), luego para minimimizar el lagrangiano se tiene que \n",
    "\n",
    "$$\\begin{align}\n",
    "&\\delta_w \\mathcal{L}(w^*,b^*,\\lambda^*)=w^*-\\sum_{i=1}^n\\lambda_i y_ix_i=0\n",
    "&\\delta_b \\mathcal{L}(w^*,b^*,\\lambda^*)=\\sum_{i=1}^n\\lambda_i y_i=0\n",
    "\\end{align}$$\n",
    "En consecuencia $w^*$ tiene la forma $w^*=\\sum_{i=1}^n\\lambda_i y_ix_i$ bajo la condición $\\sum_{i=1}^n\\lambda_i y_i=0$\n",
    "\n",
    "De modo que la función dual nos quedaría \n",
    "\n",
    "$$\\begin{align}\n",
    "g(\\lambda)&=\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n\\lambda_i \\lambda_j y_iy_jx_i^Tx_j-\\sum_{i=1}^n \\sum_{j=1}^n\\lambda_i \\lambda_j y_iy_jx_i^Tx_j+\\sum_{i=1}^n \\lambda_i - b\\sum_{i=1}^n \\lambda_iy_i \\\\\n",
    "&=\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n\\lambda_i \\lambda_j y_iy_jx_i^Tx_j-\\sum_{i=1}^n \\sum_{j=1}^n\\lambda_i \\lambda_j y_iy_jx_i^Tx_j+\\sum_{i=1}^n \\lambda_i \\\\\n",
    "&=-\\frac{1}{2}\\sum_{i=1}^n \\sum_{j=1}^n\\lambda_i \\lambda_j y_iy_jx_i^Tx_j+\\sum_{i=1}^n \\lambda_i \n",
    "\\end{align}$$\n",
    "\n",
    "#### Problema dual\n",
    "$$\\begin{align}\n",
    "&\\max_{\\lambda} g(\\lambda)= -\\frac{1}{2}\\lambda^T A \\lambda +\\lambda^T 1\\\\ \n",
    "&sujeto \\ a \\\\\n",
    "&\\lambda^T y=0\\\\\n",
    "& \\lambda \\succeq 0\n",
    "\\end{align}$$\n",
    "\n",
    "Donde $A$ es una matriz simétrica de tamaño $n\\times n$ con componentes $a_{ij}=y_iy_jx_iTx_j$, de esta manera nos queda un problema de optimización convexa $QP$.\n",
    "\n",
    "Así pues, solucionando el problema dual encontramos la solución exacta al problema original, debido a que se tiene dualidad fuerte, el gap de dualidad es cero, así mismo, se cumplen las condiciones de $KKT$, en particular la de \"Complementary slackness\", es decir \n",
    "\n",
    "$\\lambda_if_i(w)=0$, $i=1\\dots ,n$\n",
    "\n",
    "Por consiguiente, obtenemos que $\\lambda_i$ es cero, o $f_i(w)=0$. Si $f_i(w)=0$ entonces $y_i(w^Tx_i+b)=1$, a los $x_i$ que cumplan esto los llamaremos vectores soporte, la ventaja de usar el problema dual reside en la anterior propiedad, pues por lo general la mayoría de los vectores $x_i$ no son vectores soporte por lo que los $\\lambda_i$ van a ser cero, reduciendo considerablemente la complejidad del problema dual.\n",
    "\n",
    "Encontrando la solución al problema dual, hallamos $w^*=\\sum_{i=1}^n\\lambda_i y_ix_i$, finalmente, para encontrar $b^*$ basta tomar un vector soporte y despejar $b*$ con el $w^*$ encontrado, y de esta manera encontramos la solución al problema. Para clasificar un nuevo vector $x$ miramos a que lado del hiperplano queda, es decir, si $w^{T}x+b<0$ entonces es benigno, si es mayor que cero entonces es maligno.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones Kernel\n",
    "Al principio supusimos que nuestros vectores se podian separar mediante un hiperplano, sin embargo, en nuestro caso esto no se tiene de entrada, por lo cual es necesario aplicarle una función de kernel de tal manera que transforme nuestro espacio y pueda existir el hiperplano de separación.\n",
    "\n",
    "<img src=\"kernel.png\"  height=\"400\" width=\"400\" margin=\"20px\">\n",
    "\n",
    "Para nuestro caso usaremos la base radial Gaussiana, que está dada por\n",
    "\n",
    "$$K(x,z)=exp(-\\frac{\\|x-z\\|^2}{2\\sigma^2})$$\n",
    "\n",
    "De manera intuitiva la base radial Gaussiana tiene en cuenta la relación de cercania entre los vectores, y el parametro $\\sigma$ puede cambiar la importancia de esa relación.\n",
    "\n",
    "De esta manera, en nuestro modelo es necesario aplicarle esa transformación a los datos, con lo cual nos quedaría el mismo problema dual \n",
    "$$\\begin{align}\n",
    "&\\max_{\\lambda} g(\\lambda)= -\\frac{1}{2}\\lambda^T A \\lambda +\\lambda^T 1\\\\ \n",
    "&sujeto \\ a \\\\\n",
    "&\\lambda^T y=0\\\\\n",
    "& \\lambda \\succeq 0\n",
    "\\end{align}$$\n",
    "\n",
    "Pero, donde $A$ es una matriz simétrica de tamaño $n\\times n$ con componentes $a_{ij}=y_iy_jK(x_i,x_j)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Navegacion entre los notebooks)**\n",
    "\n",
    "- Ir a [presentación del problema](Presentacion_Problema.ipynb)\n",
    "\n",
    "- Ir a [Implementación](Implementacion.ipynb)\n",
    "\n",
    "---\n",
    "\n",
    "**Autores:** Alejandro Martin Salcedo, David Alexander Núñez Quintero, Paula Ximena Rodriguez Nempeque.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Modelo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
